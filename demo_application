{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Copy of Part 12 bis - Encrypted Training on MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walexi/pytorch_challenge/blob/master/demo_application\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMfy9kGrmtV6",
        "colab_type": "text"
      },
      "source": [
        "## Imports and training configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epBgLzMfmtV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL6JZ1KQmtWE",
        "colab_type": "text"
      },
      "source": [
        "This class describes all the hyper-parameters for the training. Note that they are all public here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGFEvtzZmtWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 64\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.02\n",
        "        self.seed = 1\n",
        "        self.log_interval = 1 # Log info at each batch\n",
        "        self.precision_fractional = 3\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "_ = torch.manual_seed(args.seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSgIGVpGnAmy",
        "colab_type": "code",
        "outputId": "29ae6b3b-01a2-44a6-ac2c-8d6bb63f94e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/8d/5c2d9931e1fa2f3770a2816896479e4f458558eacd60425a597164ea2c23/syft-0.1.26a1-py3-none-any.whl (291kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 24.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30kB 31.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 40kB 35.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 51kB 39.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 61kB 42.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 71kB 44.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 81kB 44.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 92kB 47.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 102kB 49.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 112kB 49.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 122kB 49.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 133kB 49.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 143kB 49.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 153kB 49.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 163kB 49.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 174kB 49.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 184kB 49.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 194kB 49.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 204kB 49.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 215kB 49.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 225kB 49.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 235kB 49.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 245kB 49.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 256kB 49.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 266kB 49.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 276kB 49.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 286kB 49.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 49.3MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.56.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 65.3MB/s \n",
            "\u001b[?25hCollecting lz4>=2.1.6 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 59.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Collecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 67.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.5)\n",
            "Collecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/4b/ad228451b1c071c5c52616b7d4298ebcfcac5ae8515ede959db19e4cd56d/websockets-8.0.2-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Collecting flask-socketio>=3.3.2 (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/66/44/edc4715af85671b943c18ac8345d0207972284a0cd630126ff5251faa08b/Flask_SocketIO-4.2.1-py2.py3-none-any.whl\n",
            "Collecting tf-encrypted!=0.5.7,>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/82/cf15aeac92525da2f794956712e7ebf418819390dec783430ee242b52d0b/tf_encrypted-0.5.8-py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 50.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Collecting zstd>=1.4.0.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/73/585134600c7fe918566adddf94af42b35cb0dad4b96ee0180190fbcbb954/zstd-1.4.3.2.tar.gz (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 62.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.56.0->syft) (1.12.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Collecting python-socketio>=4.3.0 (from flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/b0/22c3f785f23fec5c7a815f47c55d7e7946a67ae2129ff604148e939d3bdb/python_socketio-4.3.1-py2.py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 30.9MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1 (from tf-encrypted!=0.5.7,>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/e8/b3212641ee2718d556df0f23f78de8303f068fe29cdaa7a91018849582fe/PyYAML-5.1.2.tar.gz (265kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 69.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0->syft) (0.46)\n",
            "Collecting python-engineio>=3.9.0 (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/20/8e3ba16102ae2e245d70d9cb9fa48b076253fdb036dc43eea142294c2897/python_engineio-3.9.3-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 64.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (2.8.0)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zstd: filename=zstd-1.4.3.2-cp36-cp36m-linux_x86_64.whl size=1076293 sha256=ef95907ffd2593fae4a94b21608b48d23f2cc891816df416c88adcbd0b3c05a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/df/a8/405bafcabba88a18c8763694e79177e2a1bbc65ac0f6b3d728\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp36-cp36m-linux_x86_64.whl size=44104 sha256=52e89a47a8f7c42af86a55ff543b40245c9a8c5dcbd4b7732628a3b69b1d9874\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/45/dd/65f0b38450c47cf7e5312883deb97d065e030c5cca0a365030\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: websocket-client, lz4, msgpack, websockets, python-engineio, python-socketio, flask-socketio, pyyaml, tf-encrypted, zstd, syft\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.2.1 lz4-2.1.10 msgpack-0.6.1 python-engineio-3.9.3 python-socketio-4.3.1 pyyaml-5.1.2 syft-0.1.26a1 tf-encrypted-0.5.8 websocket-client-0.56.0 websockets-8.0.2 zstd-1.4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSvuHw4LmtWN",
        "colab_type": "text"
      },
      "source": [
        "Here are PySyft imports. We connect to two remote workers that be call `alice` and `bob` and request another worker called the `crypto_provider` who gives all the crypto primitives we may need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s98w7mYomtWQ",
        "colab_type": "code",
        "outputId": "d718a23b-09e3-4807-d0f4-891a9ed029ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import syft as sy  # import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
        "\n",
        "# simulation functions\n",
        "def connect_to_workers(n_workers):\n",
        "    return [\n",
        "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
        "        for i in range(n_workers)\n",
        "    ]\n",
        "def connect_to_crypto_provider():\n",
        "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
        "\n",
        "workers = connect_to_workers(n_workers=2)\n",
        "crypto_provider = connect_to_crypto_provider()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAQ3PSdum_MI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_71YeEKmtWb",
        "colab_type": "text"
      },
      "source": [
        "## Getting access and secret share data\n",
        "\n",
        "Here we're using a utility function which simulates the following behaviour: we assume the MNIST dataset is distributed in parts each of which is held by one of our workers. The workers then split their data in batches and secret share their data between each others. The final object returned is an iterable on these secret shared batches, that we call the **private data loader**. Note that during the process the local worker (so us) never had access to the data.\n",
        "\n",
        "We obtain as usual a training and testing private dataset, and both the inputs and labels are secret shared."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFD3gaWemtWe",
        "colab_type": "code",
        "outputId": "75b6c36b-8d10-481d-faa0-94933afed47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
        "n_train_items = 640\n",
        "n_test_items = 640\n",
        "\n",
        "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
        "    \n",
        "    def one_hot_of(index_tensor):\n",
        "        \"\"\"\n",
        "        Transform to one hot tensor\n",
        "        \n",
        "        Example:\n",
        "            [0, 3, 9]\n",
        "            =>\n",
        "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
        "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
        "            \n",
        "        \"\"\"\n",
        "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
        "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
        "        return onehot_tensor\n",
        "        \n",
        "    def secret_share(tensor):\n",
        "        \"\"\"\n",
        "        Transform to fixed precision and secret share a tensor\n",
        "        \"\"\"\n",
        "        return (\n",
        "            tensor\n",
        "            .fix_precision(precision_fractional=precision_fractional)\n",
        "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
        "        )\n",
        "    \n",
        "    transformation = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "    \n",
        "    private_train_loader = [\n",
        "        (secret_share(data), secret_share(one_hot_of(target)))\n",
        "        for i, (data, target) in enumerate(train_loader)\n",
        "        if i < n_train_items / args.batch_size\n",
        "    ]\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
        "        batch_size=args.test_batch_size\n",
        "    )\n",
        "    \n",
        "    private_test_loader = [\n",
        "        (secret_share(data), secret_share(target.float()))\n",
        "        for i, (data, target) in enumerate(test_loader)\n",
        "        if i < n_test_items / args.test_batch_size\n",
        "    ]\n",
        "    \n",
        "    return private_train_loader, private_test_loader\n",
        "    \n",
        "    \n",
        "private_train_loader, private_test_loader = get_private_data_loaders(\n",
        "    precision_fractional=args.precision_fractional,\n",
        "    workers=workers,\n",
        "    crypto_provider=crypto_provider\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3851286.24it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 66386.27it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 1109384.05it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 25122.19it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6V2uEeXmtWn",
        "colab_type": "text"
      },
      "source": [
        "## Model specification\n",
        "\n",
        "Here is the model that we will use, it's a rather simple one but [it has proved to perform reasonably well on MNIST](https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIA9Ix7imtWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-n6W8ANmtW0",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing functions\n",
        "\n",
        "The training is done almost as usual, the real difference is that we can't use losses like negative log-likelihood (`F.nll_loss` in PyTorch) because it's quite complicated to reproduce these functions with SMPC. Instead, we use a simpler Mean Square Error loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P66OH8d7mtW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, private_train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
        "        start_time = time.time()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(data)\n",
        "        \n",
        "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
        "        batch_size = output.shape[0]\n",
        "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            loss = loss.get().float_precision()\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
        "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
        "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu11u-ICmtXE",
        "colab_type": "text"
      },
      "source": [
        "The test function does not change!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JITe2x1lmtXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, private_test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in private_test_loader:\n",
        "            start_time = time.time()\n",
        "            \n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target.view_as(pred)).sum()\n",
        "\n",
        "    correct = correct.get().float_precision()\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
        "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI2ZDy9OmtXt",
        "colab_type": "text"
      },
      "source": [
        "### Let's launch the training !\n",
        "\n",
        "A few notes about what's happening here. First, we secret share all the model parameters across our workers. Second, we convert optimizer's hyperparameters to fixed precision. Note that we don't need to secret share them because they are public in our context, but as secret shared values live in finite fields we still need to move them in finite fields using using `.fix_precision`, in order to perform consistently operations like the weight update $W \\leftarrow W - \\alpha * \\Delta W$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HYRj-kqFmtXy",
        "colab_type": "code",
        "outputId": "1b6692d7-db19-40d3-e046-69e10ea819e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Net()\n",
        "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
        "optimizer = optimizer.fix_precision() \n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train(args, model, private_train_loader, optimizer, epoch)\n",
        "    test(args, model, private_test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/640 (0%)]\tLoss: 1.128000\tTime: 4.995s\n",
            "Train Epoch: 1 [64/640 (10%)]\tLoss: 1.011000\tTime: 5.022s\n",
            "Train Epoch: 1 [128/640 (20%)]\tLoss: 0.990000\tTime: 5.024s\n",
            "Train Epoch: 1 [192/640 (30%)]\tLoss: 0.902000\tTime: 4.977s\n",
            "Train Epoch: 1 [256/640 (40%)]\tLoss: 0.887000\tTime: 4.926s\n",
            "Train Epoch: 1 [320/640 (50%)]\tLoss: 0.875000\tTime: 4.924s\n",
            "Train Epoch: 1 [384/640 (60%)]\tLoss: 0.853000\tTime: 4.878s\n",
            "Train Epoch: 1 [448/640 (70%)]\tLoss: 0.849000\tTime: 4.870s\n",
            "Train Epoch: 1 [512/640 (80%)]\tLoss: 0.830000\tTime: 4.851s\n",
            "Train Epoch: 1 [576/640 (90%)]\tLoss: 0.839000\tTime: 4.856s\n",
            "\n",
            "Test set: Accuracy: 300.0/640 (47%)\n",
            "\n",
            "Train Epoch: 2 [0/640 (0%)]\tLoss: 0.782000\tTime: 4.846s\n",
            "Train Epoch: 2 [64/640 (10%)]\tLoss: 0.732000\tTime: 4.825s\n",
            "Train Epoch: 2 [128/640 (20%)]\tLoss: 0.794000\tTime: 4.798s\n",
            "Train Epoch: 2 [192/640 (30%)]\tLoss: 0.717000\tTime: 4.810s\n",
            "Train Epoch: 2 [256/640 (40%)]\tLoss: 0.705000\tTime: 4.862s\n",
            "Train Epoch: 2 [320/640 (50%)]\tLoss: 0.707000\tTime: 4.819s\n",
            "Train Epoch: 2 [384/640 (60%)]\tLoss: 0.703000\tTime: 4.823s\n",
            "Train Epoch: 2 [448/640 (70%)]\tLoss: 0.720000\tTime: 4.809s\n",
            "Train Epoch: 2 [512/640 (80%)]\tLoss: 0.711000\tTime: 4.852s\n",
            "Train Epoch: 2 [576/640 (90%)]\tLoss: 0.745000\tTime: 4.943s\n",
            "\n",
            "Test set: Accuracy: 462.0/640 (72%)\n",
            "\n",
            "Train Epoch: 3 [0/640 (0%)]\tLoss: 0.668000\tTime: 4.801s\n",
            "Train Epoch: 3 [64/640 (10%)]\tLoss: 0.599000\tTime: 4.811s\n",
            "Train Epoch: 3 [128/640 (20%)]\tLoss: 0.699000\tTime: 4.810s\n",
            "Train Epoch: 3 [192/640 (30%)]\tLoss: 0.601000\tTime: 4.799s\n",
            "Train Epoch: 3 [256/640 (40%)]\tLoss: 0.591000\tTime: 5.003s\n",
            "Train Epoch: 3 [320/640 (50%)]\tLoss: 0.592000\tTime: 4.806s\n",
            "Train Epoch: 3 [384/640 (60%)]\tLoss: 0.603000\tTime: 4.800s\n",
            "Train Epoch: 3 [448/640 (70%)]\tLoss: 0.629000\tTime: 4.838s\n",
            "Train Epoch: 3 [512/640 (80%)]\tLoss: 0.625000\tTime: 4.864s\n",
            "Train Epoch: 3 [576/640 (90%)]\tLoss: 0.669000\tTime: 4.864s\n",
            "\n",
            "Test set: Accuracy: 497.0/640 (78%)\n",
            "\n",
            "Train Epoch: 4 [0/640 (0%)]\tLoss: 0.582000\tTime: 4.806s\n",
            "Train Epoch: 4 [64/640 (10%)]\tLoss: 0.503000\tTime: 4.845s\n",
            "Train Epoch: 4 [128/640 (20%)]\tLoss: 0.624000\tTime: 4.817s\n",
            "Train Epoch: 4 [192/640 (30%)]\tLoss: 0.516000\tTime: 4.804s\n",
            "Train Epoch: 4 [256/640 (40%)]\tLoss: 0.514000\tTime: 4.824s\n",
            "Train Epoch: 4 [320/640 (50%)]\tLoss: 0.511000\tTime: 4.804s\n",
            "Train Epoch: 4 [384/640 (60%)]\tLoss: 0.532000\tTime: 4.876s\n",
            "Train Epoch: 4 [448/640 (70%)]\tLoss: 0.564000\tTime: 4.874s\n",
            "Train Epoch: 4 [512/640 (80%)]\tLoss: 0.559000\tTime: 4.861s\n",
            "Train Epoch: 4 [576/640 (90%)]\tLoss: 0.618000\tTime: 4.813s\n",
            "\n",
            "Test set: Accuracy: 517.0/640 (81%)\n",
            "\n",
            "Train Epoch: 5 [0/640 (0%)]\tLoss: 0.525000\tTime: 4.823s\n",
            "Train Epoch: 5 [64/640 (10%)]\tLoss: 0.441000\tTime: 4.831s\n",
            "Train Epoch: 5 [128/640 (20%)]\tLoss: 0.571000\tTime: 4.857s\n",
            "Train Epoch: 5 [192/640 (30%)]\tLoss: 0.458000\tTime: 4.822s\n",
            "Train Epoch: 5 [256/640 (40%)]\tLoss: 0.460000\tTime: 4.806s\n",
            "Train Epoch: 5 [320/640 (50%)]\tLoss: 0.453000\tTime: 4.834s\n",
            "Train Epoch: 5 [384/640 (60%)]\tLoss: 0.480000\tTime: 4.913s\n",
            "Train Epoch: 5 [448/640 (70%)]\tLoss: 0.515000\tTime: 4.846s\n",
            "Train Epoch: 5 [512/640 (80%)]\tLoss: 0.507000\tTime: 4.839s\n",
            "Train Epoch: 5 [576/640 (90%)]\tLoss: 0.576000\tTime: 4.815s\n",
            "\n",
            "Test set: Accuracy: 529.0/640 (83%)\n",
            "\n",
            "Train Epoch: 6 [0/640 (0%)]\tLoss: 0.475000\tTime: 4.808s\n",
            "Train Epoch: 6 [64/640 (10%)]\tLoss: 0.392000\tTime: 4.811s\n",
            "Train Epoch: 6 [128/640 (20%)]\tLoss: 0.528000\tTime: 4.863s\n",
            "Train Epoch: 6 [192/640 (30%)]\tLoss: 0.410000\tTime: 4.805s\n",
            "Train Epoch: 6 [256/640 (40%)]\tLoss: 0.414000\tTime: 4.860s\n",
            "Train Epoch: 6 [320/640 (50%)]\tLoss: 0.408000\tTime: 4.853s\n",
            "Train Epoch: 6 [384/640 (60%)]\tLoss: 0.438000\tTime: 4.911s\n",
            "Train Epoch: 6 [448/640 (70%)]\tLoss: 0.474000\tTime: 4.879s\n",
            "Train Epoch: 6 [512/640 (80%)]\tLoss: 0.466000\tTime: 4.837s\n",
            "Train Epoch: 6 [576/640 (90%)]\tLoss: 0.543000\tTime: 4.805s\n",
            "\n",
            "Test set: Accuracy: 539.0/640 (84%)\n",
            "\n",
            "Train Epoch: 7 [0/640 (0%)]\tLoss: 0.437000\tTime: 4.789s\n",
            "Train Epoch: 7 [64/640 (10%)]\tLoss: 0.354000\tTime: 4.804s\n",
            "Train Epoch: 7 [128/640 (20%)]\tLoss: 0.489000\tTime: 4.830s\n",
            "Train Epoch: 7 [192/640 (30%)]\tLoss: 0.374000\tTime: 4.870s\n",
            "Train Epoch: 7 [256/640 (40%)]\tLoss: 0.379000\tTime: 4.857s\n",
            "Train Epoch: 7 [320/640 (50%)]\tLoss: 0.368000\tTime: 4.831s\n",
            "Train Epoch: 7 [384/640 (60%)]\tLoss: 0.403000\tTime: 4.852s\n",
            "Train Epoch: 7 [448/640 (70%)]\tLoss: 0.442000\tTime: 4.815s\n",
            "Train Epoch: 7 [512/640 (80%)]\tLoss: 0.432000\tTime: 4.814s\n",
            "Train Epoch: 7 [576/640 (90%)]\tLoss: 0.508000\tTime: 4.852s\n",
            "\n",
            "Test set: Accuracy: 544.0/640 (85%)\n",
            "\n",
            "Train Epoch: 8 [0/640 (0%)]\tLoss: 0.405000\tTime: 4.804s\n",
            "Train Epoch: 8 [64/640 (10%)]\tLoss: 0.324000\tTime: 4.821s\n",
            "Train Epoch: 8 [128/640 (20%)]\tLoss: 0.458000\tTime: 4.830s\n",
            "Train Epoch: 8 [192/640 (30%)]\tLoss: 0.347000\tTime: 4.880s\n",
            "Train Epoch: 8 [256/640 (40%)]\tLoss: 0.352000\tTime: 4.858s\n",
            "Train Epoch: 8 [320/640 (50%)]\tLoss: 0.341000\tTime: 4.901s\n",
            "Train Epoch: 8 [384/640 (60%)]\tLoss: 0.374000\tTime: 4.923s\n",
            "Train Epoch: 8 [448/640 (70%)]\tLoss: 0.416000\tTime: 4.873s\n",
            "Train Epoch: 8 [512/640 (80%)]\tLoss: 0.402000\tTime: 4.818s\n",
            "Train Epoch: 8 [576/640 (90%)]\tLoss: 0.483000\tTime: 4.816s\n",
            "\n",
            "Test set: Accuracy: 553.0/640 (86%)\n",
            "\n",
            "Train Epoch: 9 [0/640 (0%)]\tLoss: 0.382000\tTime: 4.831s\n",
            "Train Epoch: 9 [64/640 (10%)]\tLoss: 0.303000\tTime: 4.864s\n",
            "Train Epoch: 9 [128/640 (20%)]\tLoss: 0.433000\tTime: 4.857s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6EYsO80mtYJ",
        "colab_type": "text"
      },
      "source": [
        "There you are! You just get 89% of accuracy using a tiny fraction of the MNIST dataset, using 100% encrypted training!"
      ]
    }
  ]
}